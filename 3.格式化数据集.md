https://docs.coqui.ai/en/latest/formatting_your_dataset.html

# Formatting Your Dataset æ ¼å¼åŒ–ä½ çš„æ•°æ®é›†

For training a TTS model, you need a dataset with speech recordings and transcriptions. The speech must be divided into audio clips and each clip needs transcription.

è®­ç»ƒTTSæ¨¡å‹éœ€è¦ä¸€ä¸ªåŒ…å«è¯­éŸ³å½•éŸ³å’Œè½¬å½•æ–‡æœ¬çš„æ•°æ®é›†ã€‚è¯­éŸ³å¿…é¡»è¢«åˆ†å‰²æˆéŸ³é¢‘ç‰‡æ®µï¼Œæ¯ä¸ªç‰‡æ®µéƒ½éœ€è¦è½¬å½•æ–‡æœ¬ã€‚

If you have a single audio file and you need to split it into clips, there are different open-source tools for you. We recommend Audacity. It is an open-source and free audio editing software.

å¦‚æœä½ æœ‰ä¸€ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼Œå¹¶ä¸”éœ€è¦å°†å…¶åˆ†å‰²æˆç‰‡æ®µï¼Œæœ‰å¤šç§å¼€æºå·¥å…·å¯ä¾›ä½ ä½¿ç”¨ã€‚æˆ‘ä»¬æ¨èAudacityã€‚å®ƒæ˜¯ä¸€æ¬¾å¼€æºä¸”å…è´¹çš„éŸ³é¢‘ç¼–è¾‘è½¯ä»¶ã€‚

It is also important to use a lossless audio file format to prevent compression artifacts. We recommend using wav file format.

ä½¿ç”¨æ— æŸéŸ³é¢‘æ–‡ä»¶æ ¼å¼ä»¥é˜²æ­¢å‹ç¼©å¤±çœŸä¹Ÿå¾ˆé‡è¦ã€‚æˆ‘ä»¬å»ºè®®ä½¿ç”¨wavæ–‡ä»¶æ ¼å¼ã€‚

Letâ€™s assume you created the audio clips and their transcription. You can collect all your clips in a folder. Letâ€™s call this folder wavs.

å‡è®¾ä½ å·²ç»åˆ›å»ºäº†éŸ³é¢‘ç‰‡æ®µåŠå…¶è½¬å½•æ–‡æœ¬ã€‚ä½ å¯ä»¥å°†æ‰€æœ‰ç‰‡æ®µæ”¶é›†åˆ°ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­ã€‚æˆ‘ä»¬æŠŠè¿™ä¸ªæ–‡ä»¶å¤¹å‘½åä¸ºwavsã€‚

```
/wavs
  | - audio1.wav
  | - audio2.wav
  | - audio3.wav
  ...
```
You can either create separate transcription files for each clip or create a text file that maps each audio clip to its transcription. In this file, each column must be delimited by a special character separating the audio file name, the transcription and the normalized transcription. And make sure that the delimiter is not used in the transcription text.


ä½ å¯ä»¥ä¸ºæ¯ä¸ªç‰‡æ®µåˆ›å»ºå•ç‹¬çš„è½¬å½•æ–‡ä»¶ï¼Œä¹Ÿå¯ä»¥åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œå°†æ¯ä¸ªéŸ³é¢‘ç‰‡æ®µä¸å…¶è½¬å½•å†…å®¹è¿›è¡Œæ˜ å°„ã€‚åœ¨æ­¤æ–‡ä»¶ä¸­ï¼Œæ¯ä¸€åˆ—å¿…é¡»ç”¨ä¸€ä¸ªç‰¹æ®Šå­—ç¬¦åˆ†éš”ï¼Œåˆ†åˆ«ç”¨äºåŒºåˆ†éŸ³é¢‘æ–‡ä»¶åã€è½¬å½•å†…å®¹å’Œæ ‡å‡†åŒ–è½¬å½•å†…å®¹ã€‚åŒæ—¶è¦ç¡®ä¿è¯¥åˆ†éš”ç¬¦æœªåœ¨è½¬å½•æ–‡æœ¬ä¸­ä½¿ç”¨ã€‚

We recommend the following format delimited by |. In the following example, audio1, audio2 refer to files audio1.wav, audio2.wav etc.


æˆ‘ä»¬æ¨èä»¥ä¸‹ç”±|åˆ†éš”çš„æ ¼å¼ã€‚åœ¨ä»¥ä¸‹ç¤ºä¾‹ä¸­ï¼Œaudio1ã€audio2åˆ†åˆ«æŒ‡æ–‡ä»¶audio1.wavã€audio2.wavç­‰ã€‚


```
# metadata.txt

audio1|This is my sentence.|This is my sentence.
audio2|1469 and 1470|fourteen sixty-nine and fourteen seventy
audio3|It'll be $16 sir.|It'll be sixteen dollars sir.
...
```

If you donâ€™t have normalized transcriptions, you can use the same transcription for both columns. If itâ€™s your case, we recommend to use normalization later in the pipeline, either in the text cleaner or in the phonemizer.

å¦‚æœæ‚¨æ²¡æœ‰æ ‡å‡†åŒ–çš„è½¬å½•æ–‡æœ¬ï¼Œå¯ä»¥åœ¨ä¸¤åˆ—ä¸­ä½¿ç”¨ç›¸åŒçš„è½¬å½•æ–‡æœ¬ã€‚å¦‚æœæ˜¯è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬å»ºè®®ç¨ååœ¨å¤„ç†æµç¨‹ä¸­è¿›è¡Œæ ‡å‡†åŒ–ï¼Œè¦ä¹ˆåœ¨æ–‡æœ¬æ¸…ç†å™¨ä¸­ï¼Œè¦ä¹ˆåœ¨éŸ³ç´ åŒ–å™¨ä¸­ã€‚

In the end, we have the following folder structure

æœ€åï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä»¥ä¸‹æ–‡ä»¶å¤¹ç»“æ„

```
/MyTTSDataset
      |
      | -> metadata.txt
      | -> /wavs
              | -> audio1.wav
              | -> audio2.wav
              | ...
```
The format above is taken from widely-used the LJSpeech dataset. You can also download and see the dataset. ğŸ¸TTS already provides tooling for the LJSpeech. if you use the same format, you can start training your models right away.

ä¸Šè¿°æ ¼å¼æ¥è‡ªå¹¿æ³›ä½¿ç”¨çš„LJSpeechæ•°æ®é›†ã€‚æ‚¨ä¹Ÿå¯ä»¥ä¸‹è½½å¹¶æŸ¥çœ‹è¯¥æ•°æ®é›†ã€‚ğŸ¸TTSå·²ç»ä¸ºLJSpeechæä¾›äº†å·¥å…·ã€‚å¦‚æœæ‚¨ä½¿ç”¨ç›¸åŒçš„æ ¼å¼ï¼Œå°±å¯ä»¥ç«‹å³å¼€å§‹è®­ç»ƒæ‚¨çš„æ¨¡å‹ã€‚

# Dataset Quality æ•°æ®é›†è´¨é‡
Your dataset should have good coverage of the target language. It should cover the phonemic variety, exceptional sounds and syllables. This is extremely important for especially non-phonemic languages like English.

ä½ çš„æ•°æ®é›†åº”è¯¥å¯¹ç›®æ ‡è¯­è¨€æœ‰è‰¯å¥½çš„è¦†ç›–ã€‚å®ƒåº”è¯¥æ¶µç›–éŸ³ä½å˜ä½“ã€ç‰¹æ®Šå‘éŸ³å’ŒéŸ³èŠ‚ã€‚è¿™å¯¹äºåƒè‹±è¯­è¿™æ ·çš„ééŸ³ä½è¯­è¨€æ¥è¯´å°¤ä¸ºé‡è¦ã€‚

For more info about dataset qualities and properties check our post.


æœ‰å…³æ•°æ®é›†è´¨é‡å’Œå±æ€§çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬çš„å¸–å­ã€‚

# Using Your Dataset in ğŸ¸TTS 

åœ¨ğŸ¸TTSä¸­ä½¿ç”¨æ‚¨çš„æ•°æ®é›†

After you collect and format your dataset, you need to check two things. Whether you need a formatter and a text_cleaner. The formatter loads the text file (created above) as a list and the text_cleaner performs a sequence of text normalization operations that converts the raw text into the spoken representation (e.g. converting numbers to text, acronyms, and symbols to the spoken format).

æ”¶é›†å¹¶æ ¼å¼åŒ–æ•°æ®é›†åï¼Œä½ éœ€è¦æ£€æŸ¥ä¸¤ä»¶äº‹ï¼šæ˜¯å¦éœ€è¦formatterå’Œtext_cleanerã€‚formatterå°†ï¼ˆä¸Šé¢åˆ›å»ºçš„ï¼‰æ–‡æœ¬æ–‡ä»¶åŠ è½½ä¸ºåˆ—è¡¨ï¼Œtext_cleaneræ‰§è¡Œä¸€ç³»åˆ—æ–‡æœ¬æ ‡å‡†åŒ–æ“ä½œï¼Œå°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºå£è¯­åŒ–è¡¨è¾¾ï¼ˆä¾‹å¦‚ï¼Œå°†æ•°å­—è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œå°†é¦–å­—æ¯ç¼©å†™è¯å’Œç¬¦å·è½¬æ¢ä¸ºå£è¯­å½¢å¼ï¼‰ã€‚

If you use a different dataset format than the LJSpeech or the other public datasets that ğŸ¸TTS supports, then you need to write your own formatter.

å¦‚æœä½ ä½¿ç”¨çš„æ•°æ®é›†æ ¼å¼ä¸LJSpeechæˆ–ğŸ¸TTSæ”¯æŒçš„å…¶ä»–å…¬å¼€æ•°æ®é›†ä¸åŒï¼Œé‚£ä¹ˆä½ éœ€è¦ç¼–å†™è‡ªå·±çš„æ ¼å¼åŒ–ç¨‹åºã€‚


If your dataset is in a new language or it needs special normalization steps, then you need a new text_cleaner.


å¦‚æœä½ çš„æ•°æ®é›†ä½¿ç”¨çš„æ˜¯ä¸€ç§æ–°è¯­è¨€ï¼Œæˆ–è€…éœ€è¦ç‰¹æ®Šçš„æ ‡å‡†åŒ–æ­¥éª¤ï¼Œé‚£ä¹ˆä½ å°±éœ€è¦ä¸€ä¸ªæ–°çš„text_cleanerã€‚


What you get out of a formatter is a List[Dict] in the following format.

ä»formatterä¸­å¾—åˆ°çš„æ˜¯ä»¥ä¸‹æ ¼å¼çš„List[Dict]ã€‚

```
>>> formatter(metafile_path)
[
    {"audio_file":"audio1.wav", "text":"This is my sentence.", "speaker_name":"MyDataset", "language": "lang_code"},
    {"audio_file":"audio1.wav", "text":"This is maybe a sentence.", "speaker_name":"MyDataset", "language": "lang_code"},
    ...
]
```

Each sub-list is parsed as {"<filename>", "<transcription>", "<speaker_name">]. <speaker_name> is the dataset name for single speaker datasets and it is mainly used in the multi-speaker models to map the speaker of the each sample. But for now, we only focus on single speaker datasets.


æ¯ä¸ªå­åˆ—è¡¨éƒ½è¢«è§£æä¸º{"<filename>", "<transcription>", "<speaker_name">]ã€‚<speaker_name>æ˜¯å•è¯´è¯äººæ•°æ®é›†çš„æ•°æ®é›†åç§°ï¼Œä¸»è¦ç”¨äºå¤šè¯´è¯äººæ¨¡å‹ä¸­ï¼Œä»¥æ˜ å°„æ¯ä¸ªæ ·æœ¬çš„è¯´è¯äººã€‚ä½†ç›®å‰ï¼Œæˆ‘ä»¬åªå…³æ³¨å•è¯´è¯äººæ•°æ®é›†ã€‚


The purpose of a formatter is to parse your manifest file and load the audio file paths and transcriptions. Then, the output is passed to the Dataset. It computes features from the audio signals, calls text normalization routines, and converts raw text to phonemes if needed.

formatterçš„ä½œç”¨æ˜¯è§£æä½ çš„æ¸…å•æ–‡ä»¶ï¼Œå¹¶åŠ è½½éŸ³é¢‘æ–‡ä»¶è·¯å¾„å’Œè½¬å½•æ–‡æœ¬ã€‚ç„¶åï¼Œè¾“å‡ºç»“æœä¼šè¢«ä¼ é€’ç»™Datasetã€‚æ•°æ®é›†ä¼šä»éŸ³é¢‘ä¿¡å·ä¸­è®¡ç®—ç‰¹å¾ï¼Œè°ƒç”¨æ–‡æœ¬æ ‡å‡†åŒ–ç¨‹åºï¼Œå¹¶åœ¨éœ€è¦æ—¶å°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºéŸ³ç´ ã€‚


# Loading your dataset åŠ è½½æ‚¨çš„æ•°æ®é›†

Load one of the dataset supported by ğŸ¸TTS.


åŠ è½½ä¸€ä¸ªç”±ğŸ¸TTSæ”¯æŒçš„æ•°æ®é›†ã€‚


```
from TTS.tts.configs.shared_configs import BaseDatasetConfig
from TTS.tts.datasets import load_tts_samples


# dataset config for one of the pre-defined datasets
dataset_config = BaseDatasetConfig(
    formatter="vctk", meta_file_train="", language="en-us", path="dataset-path")
)

# load training samples
train_samples, eval_samples = load_tts_samples(dataset_config, eval_split=True)
```

Load a custom dataset with a custom formatter.


ä½¿ç”¨è‡ªå®šä¹‰æ ¼å¼åŒ–ç¨‹åºåŠ è½½è‡ªå®šä¹‰æ•°æ®é›†ã€‚

```
from TTS.tts.datasets import load_tts_samples


# custom formatter implementation
def formatter(root_path, manifest_file, **kwargs):  # pylint: disable=unused-argument
    """Assumes each line as ```<filename>|<transcription>```
    """
    txt_file = os.path.join(root_path, manifest_file)
    items = []
    speaker_name = "my_speaker"
    with open(txt_file, "r", encoding="utf-8") as ttf:
        for line in ttf:
            cols = line.split("|")
            wav_file = os.path.join(root_path, "wavs", cols[0])
            text = cols[1]
            items.append({"text":text, "audio_file":wav_file, "speaker_name":speaker_name, "root_path": root_path})
    return items

# load training samples
train_samples, eval_samples = load_tts_samples(dataset_config, eval_split=True, formatter=formatter)
```
See TTS.tts.datasets.TTSDataset, a generic Dataset implementation for the tts models.

å‚è§TTS.tts.datasets.TTSDatasetï¼Œè¿™æ˜¯ä¸€ä¸ªé€‚ç”¨äºttsæ¨¡å‹çš„é€šç”¨Datasetå®ç°ã€‚


See TTS.vocoder.datasets.*, for different Dataset implementations for the vocoder models.


å‚è§TTS.vocoder.datasets.*ï¼Œäº†è§£ç”¨äºvocoderæ¨¡å‹çš„ä¸åŒDatasetå®ç°ã€‚

See TTS.utils.audio.AudioProcessor that includes all the audio processing and feature extraction functions used in a Dataset implementation. Feel free to add things as you need.


å‚è§TTS.utils.audio.AudioProcessorï¼Œå…¶ä¸­åŒ…å«åœ¨Datasetå®ç°ä¸­ä½¿ç”¨çš„æ‰€æœ‰éŸ³é¢‘å¤„ç†å’Œç‰¹å¾æå–å‡½æ•°ã€‚ä½ å¯ä»¥æ ¹æ®éœ€è¦éšæ„æ·»åŠ å†…å®¹ã€‚



