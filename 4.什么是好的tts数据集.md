https://docs.coqui.ai/en/latest/what_makes_a_good_dataset.html

# What makes a good TTS dataset ä»€ä¹ˆæ˜¯å¥½çš„TTSæ•°æ®é›†

## What Makes a Good Dataset ä»€ä¹ˆæ˜¯ä¼˜è´¨æ•°æ®é›†

- Gaussian like distribution on clip and text lengths. So plot the distribution of clip lengths and check if it covers enough short and long voice clips.
- å‰ªè¾‘å’Œæ–‡æœ¬é•¿åº¦çš„ç±»é«˜æ–¯åˆ†å¸ƒã€‚å› æ­¤ï¼Œç»˜åˆ¶å‰ªè¾‘é•¿åº¦çš„åˆ†å¸ƒï¼Œæ£€æŸ¥å®ƒæ˜¯å¦æ¶µç›–äº†è¶³å¤Ÿå¤šçš„çŸ­è¯­éŸ³å‰ªè¾‘å’Œé•¿è¯­éŸ³å‰ªè¾‘ã€‚
- Mistake free. Remove any wrong or broken files. Check annotations, compare transcript and audio length.
- é›¶é”™è¯¯ã€‚ç§»é™¤æ‰€æœ‰é”™è¯¯æˆ–æŸåçš„æ–‡ä»¶ã€‚æ£€æŸ¥æ³¨é‡Šï¼Œæ¯”è¾ƒæ–‡å­—è®°å½•å’ŒéŸ³é¢‘æ—¶é•¿ã€‚
- Noise free. Background noise might lead your model to struggle, especially for a good alignment. Even if it learns the alignment, the final result is likely to be suboptimial.
- æ— å™ªéŸ³ã€‚èƒŒæ™¯å™ªéŸ³å¯èƒ½ä¼šè®©ä½ çš„æ¨¡å‹éš¾ä»¥å¤„ç†ï¼Œå°¤å…¶æ˜¯åœ¨å®ç°è‰¯å¥½å¯¹é½æ–¹é¢ã€‚å³ä½¿æ¨¡å‹å­¦ä¼šäº†å¯¹é½ï¼Œæœ€ç»ˆç»“æœä¹Ÿå¯èƒ½ä¸å¤Ÿç†æƒ³ã€‚
- Compatible tone and pitch among voice clips. For instance, if you are using audiobook recordings for your project, it might have impersonations for different characters in the book. These differences between samples downgrade the model performance
- è¯­éŸ³ç‰‡æ®µä¹‹é—´çš„å…¼å®¹è¯­è°ƒå’ŒéŸ³é«˜ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ åœ¨é¡¹ç›®ä¸­ä½¿ç”¨æœ‰å£°ä¹¦å½•éŸ³ï¼Œå…¶ä¸­å¯èƒ½åŒ…å«å¯¹ä¹¦ä¸­ä¸åŒè§’è‰²çš„æ¨¡ä»¿ã€‚æ ·æœ¬ä¹‹é—´çš„è¿™äº›å·®å¼‚ä¼šé™ä½æ¨¡å‹æ€§èƒ½ã€‚
- Good phoneme coverage. Make sure that your dataset covers a good portion of the phonemes, di-phonemes, and in some languages tri-phonemes.
- è‰¯å¥½çš„éŸ³ç´ è¦†ç›–ã€‚ç¡®ä¿ä½ çš„æ•°æ®é›†æ¶µç›–å¤§éƒ¨åˆ†éŸ³ç´ ã€åŒéŸ³ç´ ï¼Œåœ¨æŸäº›è¯­è¨€ä¸­è¿˜è¦æ¶µç›–ä¸‰éŸ³ç´ ã€‚
- Naturalness of recordings. For your model WISIAIL (What it sees is all it learns). Therefore, your dataset should accommodate all the attributes you want to hear from your model.
- å½•éŸ³çš„è‡ªç„¶åº¦ã€‚å¯¹äºæ‚¨çš„æ¨¡å‹WISIAILï¼ˆæ‰€è§å³æ‰€å­¦ï¼‰è€Œè¨€ï¼Œæ‚¨çš„æ•°æ®é›†åº”åŒ…å«æ‰€æœ‰æ‚¨å¸Œæœ›ä»æ¨¡å‹ä¸­å¬åˆ°çš„å±æ€§ã€‚

# Preprocessing Dataset é¢„å¤„ç†æ•°æ®é›†

If you like to use a bespoken dataset, you might like to perform a couple of quality checks before training. ğŸ¸TTS provides a couple of notebooks (CheckSpectrograms, AnalyzeDataset) to expedite this part for you.

å¦‚æœä½ å–œæ¬¢ä½¿ç”¨å®šåˆ¶æ•°æ®é›†ï¼Œå¯èƒ½éœ€è¦åœ¨è®­ç»ƒå‰è¿›è¡Œä¸€äº›è´¨é‡æ£€æŸ¥ã€‚ğŸ¸TTS æä¾›äº†å‡ ä¸ªç¬”è®°æœ¬ï¼ˆCheckSpectrogramsã€AnalyzeDatasetï¼‰æ¥åŠ å¿«è¿™éƒ¨åˆ†çš„è¿›ç¨‹ã€‚

- AnalyzeDataset is for checking dataset distribution in terms of the clip and transcript lengths. It is good to find outlier instances (too long, short text but long voice clip, etc.)and remove them before training. Keep in mind that we like to have a good balance between long and short clips to prevent any bias in training. If you have only short clips (1-3 secs), then your model might suffer for long sentences and if your instances are long, then it might not learn the alignment or might take too long to train the model.

- åˆ†ææ•°æ®é›†ç”¨äºä»ç‰‡æ®µå’Œè½¬å½•æ–‡æœ¬é•¿åº¦çš„è§’åº¦æ£€æŸ¥æ•°æ®é›†åˆ†å¸ƒã€‚åœ¨è®­ç»ƒå‰ï¼Œæ‰¾å‡ºå¼‚å¸¸å®ä¾‹ï¼ˆå¦‚è¿‡é•¿ã€æ–‡æœ¬çŸ­ä½†è¯­éŸ³ç‰‡æ®µé•¿ç­‰ï¼‰å¹¶å°†å…¶ç§»é™¤æ˜¯å¾ˆæœ‰å¿…è¦çš„ã€‚è¦è®°ä½ï¼Œæˆ‘ä»¬å¸Œæœ›é•¿ç‰‡æ®µå’ŒçŸ­ç‰‡æ®µä¹‹é—´ä¿æŒè‰¯å¥½çš„å¹³è¡¡ï¼Œä»¥é¿å…è®­ç»ƒä¸­å‡ºç°ä»»ä½•åå·®ã€‚å¦‚æœåªæœ‰çŸ­ç‰‡æ®µï¼ˆ1-3ç§’ï¼‰ï¼Œæ¨¡å‹å¯èƒ½éš¾ä»¥å¤„ç†é•¿å¥å­ï¼›è€Œå¦‚æœå®ä¾‹è¿‡é•¿ï¼Œæ¨¡å‹å¯èƒ½æ— æ³•å­¦ä¹ åˆ°å¯¹é½å…³ç³»ï¼Œæˆ–è€…è®­ç»ƒæ—¶é—´ä¼šè¿‡é•¿ã€‚
- CheckSpectrograms is to measure the noise level of the clips and find good audio processing parameters. The noise level might be observed by checking spectrograms. If spectrograms look cluttered, especially in silent parts, this dataset might not be a good candidate for a TTS project. If your voice clips are too noisy in the background, it makes things harder for your model to learn the alignment, and the final result might be different than the voice you are given. If the spectrograms look good, then the next step is to find a good set of audio processing parameters, defined in config.json. In the notebook, you can compare different sets of parameters and see the resynthesis results in relation to the given ground-truth. Find the best parameters that give the best possible synthesis performance.
- CheckSpectrogramsç”¨äºæµ‹é‡ç‰‡æ®µçš„å™ªå£°æ°´å¹³å¹¶æ‰¾åˆ°è‰¯å¥½çš„éŸ³é¢‘å¤„ç†å‚æ•°ã€‚é€šè¿‡æŸ¥çœ‹é¢‘è°±å›¾å¯ä»¥è§‚å¯Ÿåˆ°å™ªå£°æ°´å¹³ã€‚å¦‚æœé¢‘è°±å›¾çœ‹èµ·æ¥æ‚ä¹±æ— ç« ï¼Œå°¤å…¶æ˜¯åœ¨é™éŸ³éƒ¨åˆ†ï¼Œé‚£ä¹ˆè¯¥æ•°æ®é›†å¯èƒ½ä¸é€‚åˆä½œä¸ºTTSé¡¹ç›®çš„å€™é€‰ã€‚å¦‚æœä½ çš„è¯­éŸ³ç‰‡æ®µèƒŒæ™¯å™ªå£°è¿‡å¤§ï¼Œä¼šä½¿æ¨¡å‹æ›´éš¾å­¦ä¹ å¯¹é½ï¼Œæœ€ç»ˆç»“æœå¯èƒ½ä¸ä½ æä¾›çš„è¯­éŸ³ä¸åŒã€‚å¦‚æœé¢‘è°±å›¾çœ‹èµ·æ¥ä¸é”™ï¼Œé‚£ä¹ˆä¸‹ä¸€æ­¥å°±æ˜¯æ‰¾åˆ°ä¸€ç»„è‰¯å¥½çš„éŸ³é¢‘å¤„ç†å‚æ•°ï¼Œè¿™äº›å‚æ•°åœ¨config.jsonä¸­å®šä¹‰ã€‚åœ¨ç¬”è®°æœ¬ä¸­ï¼Œä½ å¯ä»¥æ¯”è¾ƒä¸åŒçš„å‚æ•°é›†ï¼Œå¹¶æŸ¥çœ‹ä¸ç»™å®šçœŸå®å€¼ç›¸å…³çš„é‡æ–°åˆæˆç»“æœã€‚æ‰¾åˆ°èƒ½å¸¦æ¥æœ€ä½³åˆæˆæ€§èƒ½çš„å‚æ•°ã€‚

Another practical detail is the quantization level of the clips. If your dataset has a very high bit-rate, that might cause slow data-load time and consequently slow training. It is better to reduce the sample-rate of your dataset to around 16000-22050.

å¦ä¸€ä¸ªå®é™…ç»†èŠ‚æ˜¯ç‰‡æ®µçš„é‡åŒ–çº§åˆ«ã€‚å¦‚æœä½ çš„æ•°æ®é›†æ¯”ç‰¹ç‡éå¸¸é«˜ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ•°æ®åŠ è½½æ—¶é—´å˜æ…¢ï¼Œè¿›è€Œä½¿è®­ç»ƒé€Ÿåº¦å‡æ…¢ã€‚æœ€å¥½å°†æ•°æ®é›†çš„é‡‡æ ·ç‡é™ä½åˆ°16000-22050å·¦å³ã€‚
